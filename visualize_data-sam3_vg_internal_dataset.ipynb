{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image\n",
    "import pycocotools.mask as mask_util\n",
    "import cv2\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The drawing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_box(boxes, colors):\n",
    "    for box, color in zip(boxes, colors):\n",
    "        x0, y0 = box[0], box[1]\n",
    "        w, h = box[2], box[3]\n",
    "        ax = plt.gca()\n",
    "        ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=color, facecolor=(0,0,0,0), lw=2))   \n",
    "\n",
    "def show_anns(masks, colors, borders=True) -> None:\n",
    "    \"\"\"\n",
    "    show the annotations\n",
    "    \"\"\"\n",
    "    # return if no masks\n",
    "    if len(masks) == 0:\n",
    "        return\n",
    "\n",
    "    # sort masks by size\n",
    "    sorted_annot_and_color = sorted(\n",
    "        zip(masks, colors), key=(lambda x: x[0].sum()), reverse=True\n",
    "    )\n",
    "    H, W = sorted_annot_and_color[0][0].shape[0], sorted_annot_and_color[0][0].shape[1]\n",
    "\n",
    "    canvas = np.ones((H, W, 4))\n",
    "    canvas[:, :, 3] = 0  # set the alpha channel\n",
    "    contour_thickness = max(1, int(min(5, 0.01 * min(H, W))))\n",
    "    for mask, color in sorted_annot_and_color:\n",
    "        canvas[mask] = np.concatenate([color, [0.55]])\n",
    "        if borders:\n",
    "            contours, _ = cv2.findContours(\n",
    "                np.array(mask, dtype=np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE\n",
    "            )\n",
    "            cv2.drawContours(\n",
    "                canvas, contours, -1, (0.05, 0.05, 0.05, 1), thickness=contour_thickness\n",
    "            )\n",
    "\n",
    "    ax = plt.gca()\n",
    "    ax.imshow(canvas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the path\n",
    "path to the folder with the video frames and path to the json annotation file (ytvis format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_dir = \"/fsx-onevision/shared/data/ta_data_center/definedAI_release/unified/train/JPEGImages_blurred_v0\"\n",
    "json_annotation = \"/fsx-onevision-auto-sync/ythu/sam3/video_grounding/release_20250131/ytvis/balanced/eval_set_v1_20250131_200_pairs_ytvis_format.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(json_annotation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is in YT-VIS format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['videos'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['categories'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [pair for pair in data['video_np_pairs'] if pair['num_tracklets_dedup'] > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = random.sample(pairs, 1)[0]\n",
    "video = [vid for vid in data['videos'] if vid['id'] == pair['video_id']][0]\n",
    "annotations = [annot for annot in data['annotations'] if annot['video_id'] == pair[\"video_id\"] and annot[\"noun_phrase\"] == pair['noun_phrase']]\n",
    "\n",
    "colors = np.random.random((len(annotations), 3))\n",
    "\n",
    "for fid in range(video['length']):\n",
    "    \n",
    "    masks = []\n",
    "    bboxes = []\n",
    "    for tracklet in annotations:\n",
    "        if tracklet['segmentations'][fid] is not None:\n",
    "            m = mask_util.decode(tracklet['segmentations'][fid]) > 0\n",
    "            box = tracklet[\"bboxes\"][fid]\n",
    "        else:\n",
    "            m = np.zeros((video[\"height\"], video[\"width\"])) > 0\n",
    "            box = [0, 0, 0, 0]\n",
    "\n",
    "        masks.append(m)\n",
    "        bboxes.append(box)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    img = Image.open(f\"{video_dir}/{video['file_names'][fid]}\")\n",
    "    plt.imshow(img)\n",
    "    show_anns(masks,colors)\n",
    "    show_box(bboxes, colors)\n",
    "    plt.title(f\"frame={video['file_names'][fid]}, np={pair['noun_phrase']}\")\n",
    "    plt.pause(0.1)  # Adjust the pause duration as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onevision_ta_2_pseudo_labeling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
