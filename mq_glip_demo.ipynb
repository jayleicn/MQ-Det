{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e5f903-72ef-415c-bdcd-2782ea8bd743",
   "metadata": {},
   "source": [
    "# Run Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "740acb89-41e8-46c2-a80f-8630fca4db8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/layers/deform_conv.py:313: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/layers/deform_conv.py:378: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/layers/deform_conv.py:428: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/layers/set_loss.py:144: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/layers/set_loss.py:326: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/home/jielei/miniconda/envs/ov_det/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jielei/miniconda/envs/ov_det/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/rpn/loss.py:96: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/rpn/loss.py:231: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/rpn/loss.py:452: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/rpn/loss.py:849: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/roi_heads/box_head/inference.py:37: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/roi_heads/box_head/loss.py:122: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/roi_heads/box_head/box_head.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# setup config\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from maskrcnn_benchmark.data import make_data_loader\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from maskrcnn_benchmark.data import transforms as T\n",
    "from maskrcnn_benchmark.config import try_to_find\n",
    "from maskrcnn_benchmark.structures.bounding_box import BoxList\n",
    "from maskrcnn_benchmark.utils.miscellaneous import mkdir, save_config\n",
    "from maskrcnn_benchmark.modeling.detector import build_detection_model\n",
    "from maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "from maskrcnn_benchmark.data.transforms import build_transforms # input image and box transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a0d136f5-707a-446e-9ac6-7527e36e620e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maskrcnn_benchmark.engine.inference import create_queries_and_maps_from_dataset, resize_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a90ea9e-18bc-487f-8501-49446b67cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set other args -- \n",
    "def get_cfg():\n",
    "    \"\"\"MQ-GLIP-L config\"\"\"\n",
    "    from maskrcnn_benchmark.config import cfg\n",
    "    cfg.local_rank = 0\n",
    "    cfg.num_gpus = 1\n",
    "    config_file = \"configs/pretrain/mq-glip-l.yaml\"\n",
    "    cfg.merge_from_file(config_file)\n",
    "    additional_model_config = \"configs/vision_query_5shot/lvis_minival_L.yaml\"\n",
    "    cfg.merge_from_file(additional_model_config)\n",
    "    \n",
    "    opts = [\n",
    "        \"MODEL.WEIGHT\", \"MODEL/mq-glip-l\",\n",
    "        \"TEST.IMS_PER_BATCH\", \"1\",\n",
    "        \"VISION_QUERY.QUERY_BANK_PATH\", \"MODEL/lvis_query_5_pool7_sel_large.pth\",\n",
    "    ]\n",
    "    cfg.merge_from_list(opts)\n",
    "    \n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "def build_model(cfg):\n",
    "    model = build_detection_model(cfg)\n",
    "    model.to(cfg.MODEL.DEVICE)\n",
    "    checkpointer = DetectronCheckpointer(cfg, model, save_dir=cfg.OUTPUT_DIR)\n",
    "    _ = checkpointer.load(cfg.MODEL.WEIGHT, force=True)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd350acb-235e-4fdf-a68f-a3ec79f775c4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISION BACKBONE USE GRADIENT CHECKPOINTING:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jielei/miniconda/envs/ov_det/lib/python3.10/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LANGUAGE BACKBONE USE GRADIENT CHECKPOINTING:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of QVBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['encoder.qv_layer.0.attn.norm.bias', 'encoder.qv_layer.0.attn.norm.weight', 'encoder.qv_layer.0.attn.norm_kv.bias', 'encoder.qv_layer.0.attn.norm_kv.weight', 'encoder.qv_layer.0.attn.to_kv.weight', 'encoder.qv_layer.0.attn.to_out.weight', 'encoder.qv_layer.0.attn.to_q.weight', 'encoder.qv_layer.0.attn_gate.linear1.weight', 'encoder.qv_layer.0.attn_gate.linear2.weight', 'encoder.qv_layer.0.attn_gate.norm.bias', 'encoder.qv_layer.0.attn_gate.norm.weight', 'encoder.qv_layer.0.ff.linear1.weight', 'encoder.qv_layer.0.ff.linear2.weight', 'encoder.qv_layer.0.ff.norm.bias', 'encoder.qv_layer.0.ff.norm.weight', 'encoder.qv_layer.0.ff_gate', 'encoder.qv_layer.1.attn.norm.bias', 'encoder.qv_layer.1.attn.norm.weight', 'encoder.qv_layer.1.attn.norm_kv.bias', 'encoder.qv_layer.1.attn.norm_kv.weight', 'encoder.qv_layer.1.attn.to_kv.weight', 'encoder.qv_layer.1.attn.to_out.weight', 'encoder.qv_layer.1.attn.to_q.weight', 'encoder.qv_layer.1.attn_gate.linear1.weight', 'encoder.qv_layer.1.attn_gate.linear2.weight', 'encoder.qv_layer.1.attn_gate.norm.bias', 'encoder.qv_layer.1.attn_gate.norm.weight', 'encoder.qv_layer.1.ff.linear1.weight', 'encoder.qv_layer.1.ff.linear2.weight', 'encoder.qv_layer.1.ff.norm.bias', 'encoder.qv_layer.1.ff.norm.weight', 'encoder.qv_layer.1.ff_gate', 'encoder.qv_layer.2.attn.norm.bias', 'encoder.qv_layer.2.attn.norm.weight', 'encoder.qv_layer.2.attn.norm_kv.bias', 'encoder.qv_layer.2.attn.norm_kv.weight', 'encoder.qv_layer.2.attn.to_kv.weight', 'encoder.qv_layer.2.attn.to_out.weight', 'encoder.qv_layer.2.attn.to_q.weight', 'encoder.qv_layer.2.attn_gate.linear1.weight', 'encoder.qv_layer.2.attn_gate.linear2.weight', 'encoder.qv_layer.2.attn_gate.norm.bias', 'encoder.qv_layer.2.attn_gate.norm.weight', 'encoder.qv_layer.2.ff.linear1.weight', 'encoder.qv_layer.2.ff.linear2.weight', 'encoder.qv_layer.2.ff.norm.bias', 'encoder.qv_layer.2.ff.norm.weight', 'encoder.qv_layer.2.ff_gate', 'encoder.qv_layer.3.attn.norm.bias', 'encoder.qv_layer.3.attn.norm.weight', 'encoder.qv_layer.3.attn.norm_kv.bias', 'encoder.qv_layer.3.attn.norm_kv.weight', 'encoder.qv_layer.3.attn.to_kv.weight', 'encoder.qv_layer.3.attn.to_out.weight', 'encoder.qv_layer.3.attn.to_q.weight', 'encoder.qv_layer.3.attn_gate.linear1.weight', 'encoder.qv_layer.3.attn_gate.linear2.weight', 'encoder.qv_layer.3.attn_gate.norm.bias', 'encoder.qv_layer.3.attn_gate.norm.weight', 'encoder.qv_layer.3.ff.linear1.weight', 'encoder.qv_layer.3.ff.linear2.weight', 'encoder.qv_layer.3.ff.norm.bias', 'encoder.qv_layer.3.ff.norm.weight', 'encoder.qv_layer.3.ff_gate', 'encoder.qv_layer.4.attn.norm.bias', 'encoder.qv_layer.4.attn.norm.weight', 'encoder.qv_layer.4.attn.norm_kv.bias', 'encoder.qv_layer.4.attn.norm_kv.weight', 'encoder.qv_layer.4.attn.to_kv.weight', 'encoder.qv_layer.4.attn.to_out.weight', 'encoder.qv_layer.4.attn.to_q.weight', 'encoder.qv_layer.4.attn_gate.linear1.weight', 'encoder.qv_layer.4.attn_gate.linear2.weight', 'encoder.qv_layer.4.attn_gate.norm.bias', 'encoder.qv_layer.4.attn_gate.norm.weight', 'encoder.qv_layer.4.ff.linear1.weight', 'encoder.qv_layer.4.ff.linear2.weight', 'encoder.qv_layer.4.ff.norm.bias', 'encoder.qv_layer.4.ff.norm.weight', 'encoder.qv_layer.4.ff_gate', 'encoder.qv_layer.5.attn.norm.bias', 'encoder.qv_layer.5.attn.norm.weight', 'encoder.qv_layer.5.attn.norm_kv.bias', 'encoder.qv_layer.5.attn.norm_kv.weight', 'encoder.qv_layer.5.attn.to_kv.weight', 'encoder.qv_layer.5.attn.to_out.weight', 'encoder.qv_layer.5.attn.to_q.weight', 'encoder.qv_layer.5.attn_gate.linear1.weight', 'encoder.qv_layer.5.attn_gate.linear2.weight', 'encoder.qv_layer.5.attn_gate.norm.bias', 'encoder.qv_layer.5.attn_gate.norm.weight', 'encoder.qv_layer.5.ff.linear1.weight', 'encoder.qv_layer.5.ff.linear2.weight', 'encoder.qv_layer.5.ff.norm.bias', 'encoder.qv_layer.5.ff.norm.weight', 'encoder.qv_layer.5.ff_gate', 'pre_select.layers.0.ff.linear1.weight', 'pre_select.layers.0.ff.linear2.weight', 'pre_select.layers.0.ff.norm.bias', 'pre_select.layers.0.ff.norm.weight', 'pre_select.layers.0.image_condition.norm.bias', 'pre_select.layers.0.image_condition.norm.weight', 'pre_select.layers.0.image_condition.norm_kv.bias', 'pre_select.layers.0.image_condition.norm_kv.weight', 'pre_select.layers.0.image_condition.to_kv.weight', 'pre_select.layers.0.image_condition.to_out.weight', 'pre_select.layers.0.image_condition.to_q.weight', 'pre_select.layers.1.ff.linear1.weight', 'pre_select.layers.1.ff.linear2.weight', 'pre_select.layers.1.ff.norm.bias', 'pre_select.layers.1.ff.norm.weight', 'pre_select.layers.1.image_condition.norm.bias', 'pre_select.layers.1.image_condition.norm.weight', 'pre_select.layers.1.image_condition.norm_kv.bias', 'pre_select.layers.1.image_condition.norm_kv.weight', 'pre_select.layers.1.image_condition.to_kv.weight', 'pre_select.layers.1.image_condition.to_out.weight', 'pre_select.layers.1.image_condition.to_q.weight', 'pre_select.layers.1.res_mapping.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers unloaded with pre-trained weight: \n",
      "rpn.head.cls_logits.{bias, weight}\n"
     ]
    }
   ],
   "source": [
    "cfg = get_cfg()\n",
    "model = build_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e244304b-dc81-48db-ac69-df218a55011a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined datasets are: ('lvis_evaluation_mini_val',).\n",
      "Loading annotations.\n",
      "Done (t=0.77s)\n",
      "Creating index.\n",
      "Index created.\n",
      "lvis_evaluation_mini_val has the 4809 data points LvisDetection\n"
     ]
    }
   ],
   "source": [
    "iou_types = (\"bbox\",)\n",
    "data_loader = make_data_loader(cfg, is_train=False, is_distributed=False)[0]\n",
    "dataset = data_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d93079fa-c16d-42ca-adcf-fe0a6c1d9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(cfg.MODEL.DEVICE)\n",
    "cpu_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79745269-670a-4931-939d-35b8927d2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e54c318f-88d2-4cc7-9c9d-5e8dcb33fb8d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data: (<maskrcnn_benchmark.structures.image_list.ImageList object at 0x73981c104d90>, ({'boxes': tensor([[606.9400, 303.3400, 639.0000, 356.8600],\n",
      "        [166.8900, 233.5800, 185.3500, 265.4400],\n",
      "        [398.5100, 205.1000, 405.6900, 215.2200],\n",
      "        [550.2200, 309.1400, 583.8000, 400.2400],\n",
      "        [240.8800, 197.0000, 252.0500, 211.8700],\n",
      "        [338.8600, 199.6600, 347.3100, 214.0500],\n",
      "        [351.1400, 207.7800, 360.5100, 229.7400],\n",
      "        [  0.0000,  38.9300, 172.1600, 128.8500],\n",
      "        [512.5800, 225.7200, 539.6500, 274.2700],\n",
      "        [480.1000, 223.5200, 491.0800, 277.0700],\n",
      "        [466.6700, 222.9000, 479.0300, 278.6700],\n",
      "        [479.4500, 171.7000, 490.7400, 222.1200],\n",
      "        [511.4800, 141.7700, 524.3900, 173.9000],\n",
      "        [  0.0000, 276.4800, 133.9300, 333.2600],\n",
      "        [489.1100, 172.5700, 511.5500, 283.0500],\n",
      "        [562.4900, 212.2500, 639.0000, 283.0200],\n",
      "        [555.9100, 206.4100, 639.0000, 288.3400],\n",
      "        [  5.4100, 164.9500, 154.0800, 262.2200]]), 'labels': tensor([  61, 1139, 1139, 1139, 1139, 1139, 1139,  181,  181,  181,  181,  181,\n",
      "         181,  181,  421,  444, 1077, 1077]), 'image_id': tensor([139]), 'area': tensor([ 1516.2300,   522.5500,    45.5800,  2105.0901,   151.3000,    99.5500,\n",
      "          203.1000, 11240.3701,  1311.8800,   637.0500,   727.8800,   600.4300,\n",
      "          418.0200,  6041.7900,  2230.0400,  4859.2798,  6525.9502, 13515.6396]), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'orig_size': tensor([426, 640]), 'size': tensor([426, 640]), 'original_od_label': tensor([-10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10, -10,\n",
      "        -10, -10, -10, -10])},), (0,), None, None)\n"
     ]
    }
   ],
   "source": [
    "# Get a single batch\n",
    "batch_iterator = iter(data_loader)\n",
    "batch_data = next(batch_iterator)\n",
    "print(\"Batch data:\", batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4124aa1-7fc6-4c20-a6cd-701c33ac504b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.tensors.shape torch.Size([1, 3, 800, 1216])\n",
      "len(targets) 1\n",
      "targets[0]['boxes'].shape torch.Size([18, 4])\n",
      "targets[0]['labels'].shape torch.Size([18])\n",
      "image_ids (0,)\n"
     ]
    }
   ],
   "source": [
    "images, targets, image_ids, *_ = batch_data\n",
    "print(f\"images.tensors.shape {images.tensors.shape}\")\n",
    "print(f\"len(targets) {len(targets)}\")\n",
    "print(f\"targets[0]['boxes'].shape {targets[0]['boxes'].shape}\")\n",
    "print(f\"targets[0]['labels'].shape {targets[0]['labels'].shape}\")\n",
    "print(f\"image_ids {image_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54bfcecc-d209-4fb5-999a-fa75d50f8f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "all_queries, all_positive_map_label_to_token = \\\n",
    "    create_queries_and_maps_from_dataset(\n",
    "        dataset, cfg, disable_print=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1fb8203-0cb4-4de7-9d08-a1d0194a876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_queries), 31\n",
      "all_queries[0], aerosol can. air conditioner. airplane. alarm clock. alcohol. alligator. almond. ambulance. amplifier. anklet. antenna. apple. applesauce. apricot. apron. aquarium. arctic . armband. armchair. armoire. armor. artichoke. trash can. ashtray. asparagus. atomizer. avocado. award. awning. ax. baboon. baby buggy. basketball backboard. backpack. handbag. suitcase. bagel. bagpipe. baguet. bait\n"
     ]
    }
   ],
   "source": [
    "# all_queries are the concatenation of all query text\n",
    "print(f\"len(all_queries), {len(all_queries)}\")\n",
    "print(f\"all_queries[0], {all_queries[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91bb5bed-8ba7-45d5-9bee-448a5151f2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_positive_map_label_to_token) 31\n",
      "len(all_positive_map_label_to_token[0]) 40\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(all_positive_map_label_to_token) {len(all_positive_map_label_to_token)}\")\n",
    "print(f\"len(all_positive_map_label_to_token[0]) {len(all_positive_map_label_to_token[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a12606a3-027a-4de1-b6d8-1d39af5293b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_i = 0\n",
    "captions = [all_queries[query_i] for ii in range(len(targets))]\n",
    "positive_map_label_to_token = all_positive_map_label_to_token[query_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f656555-d3a7-4e5b-a3dd-426fbce2b9cd",
   "metadata": {},
   "source": [
    "### Run inference with internal forward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6b29ef5a-b907-4250-875e-b122b9e1ccc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jielei/miniconda/envs/ov_det/lib/python3.10/site-packages/transformers/modeling_utils.py:1614: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/rpn/vldyhead.py:224: UserWarning: `nn.functional.upsample_bilinear` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  temp_fea.append(F.upsample_bilinear(self.DyConv[0](visual_feats[level + 1], **conv_args),\n"
     ]
    }
   ],
   "source": [
    "output = model(\n",
    "    images.to(device), \n",
    "    captions=captions, \n",
    "    positive_map=positive_map_label_to_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b7e2080-3892-4b55-9635-c63fc4693a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [o.to(cpu_device) for o in output]\n",
    "# len(output) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d8d177ba-f1c4-4156-8db9-1ddbab2196a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdetr_style_output = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bae4bdf-aea8-4620-a289-19631c1658c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mdetr_style_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m labels \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mextra_fields[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      5\u001b[0m boxes \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mbbox\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmdetr_style_output\u001b[49m\u001b[38;5;241m.\u001b[39mappend((targets[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitem(), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscores\u001b[39m\u001b[38;5;124m\"\u001b[39m: scores, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m: boxes}))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mdetr_style_output' is not defined"
     ]
    }
   ],
   "source": [
    "# output = output[0]\n",
    "output = resize_box(output, targets)\n",
    "scores = output.extra_fields[\"scores\"]\n",
    "labels = output.extra_fields[\"labels\"]\n",
    "boxes = output.bbox\n",
    "mdetr_style_output.append((targets[0][\"image_id\"].item(), {\"scores\": scores, \"labels\": labels, \"boxes\": boxes}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95b2a59d-f61b-48f6-94d2-1559103cded2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a65006ca-8a30-43a1-8581-ec7be42b9f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2666, 0.2419, 0.2098, 0.1999, 0.2339, 0.2872, 0.5625, 0.2698, 0.2341,\n",
       "        0.1572, 0.1952, 0.1874, 0.1961, 0.3257, 0.2210, 0.2717, 0.2681, 0.1921,\n",
       "        0.1942, 0.2086, 0.2601, 0.2066, 0.2483, 0.3237, 0.3611, 0.4103, 0.5202,\n",
       "        0.2265, 0.2481, 0.1909, 0.1836, 0.2003, 0.2370, 0.2585, 0.2503, 0.3306,\n",
       "        0.5526, 0.3465, 0.2616, 0.3472, 0.2737, 0.2313, 0.1800, 0.3191, 0.1800,\n",
       "        0.2381, 0.2223, 0.2420, 0.2628, 0.1864, 0.1883, 0.1398, 0.2173, 0.2172,\n",
       "        0.3725, 0.1822, 0.2366, 0.2352, 0.1973, 0.3507, 0.3331, 0.2312, 0.4629,\n",
       "        0.1637, 0.1980, 0.2136, 0.2176, 0.1989, 0.2993, 0.2361, 0.3761, 0.3259,\n",
       "        0.2289, 0.3363, 0.2754, 0.2352, 0.4140, 0.2154, 0.2961, 0.2177, 0.2608,\n",
       "        0.2092, 0.1456, 0.1459, 0.2619, 0.1689, 0.1640, 0.2152, 0.4973, 0.2288],\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e5e50af2-7f31-4bb3-b8b4-88e66ada80a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29c7c2ed-344e-4221-a1a5-3c92312289a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2, 11, 12, 20,  2,  9, 20, 20, 20,  1, 15, 20,  5, 15, 12, 20,  9, 20,\n",
       "        36,  5,  5, 15,  1,  5, 20, 20, 20, 15, 20, 20, 10, 35,  9, 19, 23, 34,\n",
       "        35, 36, 19, 23, 35, 36,  9, 19, 35, 23, 35,  9, 36, 35, 35, 35,  2, 20,\n",
       "        20, 15, 20, 20, 19, 20, 15, 20, 20, 20, 15, 20, 20, 20, 20, 19, 20, 20,\n",
       "        20,  5,  2,  9, 20, 20,  9, 19, 20, 20, 15, 19, 20, 20, 20, 20, 20, 20])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "85df082e-f299-4706-a148-c81050baf07c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6455df2-483a-4693-b47f-2d40bc43aba8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.0545e+02, 8.3904e+01, 5.3385e+02, 1.3484e+02],\n",
       "        [5.0545e+02, 8.3904e+01, 5.3385e+02, 1.3484e+02],\n",
       "        [4.4787e+02, 1.2042e+02, 4.6073e+02, 1.4120e+02],\n",
       "        [4.7615e+02, 1.3679e+02, 5.0348e+02, 1.7144e+02],\n",
       "        [4.7603e+02, 1.3669e+02, 5.2467e+02, 1.7442e+02],\n",
       "        [4.7603e+02, 1.3669e+02, 5.2467e+02, 1.7442e+02],\n",
       "        [4.7603e+02, 1.3669e+02, 5.2467e+02, 1.7442e+02],\n",
       "        [4.9537e+02, 1.3778e+02, 5.2486e+02, 1.7410e+02],\n",
       "        [5.0973e+02, 1.4198e+02, 5.2443e+02, 1.7383e+02],\n",
       "        [4.9246e+02, 1.5416e+02, 5.0109e+02, 1.7234e+02],\n",
       "        [4.2731e+02, 1.7353e+02, 4.5004e+02, 2.2629e+02],\n",
       "        [4.7877e+02, 1.7135e+02, 4.9130e+02, 2.2235e+02],\n",
       "        [3.1363e+02, 1.9161e+02, 3.2378e+02, 2.1420e+02],\n",
       "        [4.2980e+02, 1.7344e+02, 4.6224e+02, 2.3188e+02],\n",
       "        [3.9813e+02, 2.0486e+02, 4.0642e+02, 2.1618e+02],\n",
       "        [4.9144e+02, 1.7289e+02, 5.1195e+02, 2.4199e+02],\n",
       "        [5.1302e+02, 2.0552e+02, 5.2655e+02, 2.2100e+02],\n",
       "        [5.1302e+02, 2.0552e+02, 5.2655e+02, 2.2100e+02],\n",
       "        [5.1303e+02, 2.0563e+02, 5.2661e+02, 2.2102e+02],\n",
       "        [3.5146e+02, 2.1045e+02, 3.6085e+02, 2.3039e+02],\n",
       "        [3.6168e+02, 2.1354e+02, 3.7319e+02, 2.3115e+02],\n",
       "        [4.3912e+02, 1.7807e+02, 4.6215e+02, 2.9239e+02],\n",
       "        [1.6693e+02, 2.3308e+02, 1.8574e+02, 2.6638e+02],\n",
       "        [1.6693e+02, 2.3308e+02, 1.8574e+02, 2.6638e+02],\n",
       "        [5.3845e+02, 2.2198e+02, 5.4918e+02, 2.7422e+02],\n",
       "        [4.5880e+02, 2.2329e+02, 4.9148e+02, 2.8027e+02],\n",
       "        [5.1204e+02, 2.2370e+02, 5.4041e+02, 2.8126e+02],\n",
       "        [4.4489e+02, 2.2756e+02, 4.6369e+02, 2.9434e+02],\n",
       "        [4.9021e+02, 2.4016e+02, 5.1195e+02, 2.8311e+02],\n",
       "        [4.0712e+02, 2.6082e+02, 4.3665e+02, 2.8429e+02],\n",
       "        [4.4678e+02, 2.8833e+02, 4.6428e+02, 2.9472e+02],\n",
       "        [6.0754e+02, 2.8690e+02, 6.3887e+02, 3.0755e+02],\n",
       "        [2.1148e+02, 2.9968e+02, 2.5586e+02, 3.2722e+02],\n",
       "        [2.1148e+02, 2.9968e+02, 2.5586e+02, 3.2722e+02],\n",
       "        [2.1148e+02, 2.9968e+02, 2.5586e+02, 3.2722e+02],\n",
       "        [2.1148e+02, 2.9968e+02, 2.5586e+02, 3.2722e+02],\n",
       "        [2.1148e+02, 2.9968e+02, 2.5586e+02, 3.2722e+02],\n",
       "        [2.1148e+02, 2.9968e+02, 2.5586e+02, 3.2722e+02],\n",
       "        [5.5516e+02, 2.9090e+02, 5.7565e+02, 3.3127e+02],\n",
       "        [5.5516e+02, 2.9090e+02, 5.7565e+02, 3.3127e+02],\n",
       "        [5.5516e+02, 2.9090e+02, 5.7565e+02, 3.3127e+02],\n",
       "        [5.5516e+02, 2.9090e+02, 5.7565e+02, 3.3127e+02],\n",
       "        [5.5511e+02, 2.9086e+02, 5.7591e+02, 3.3125e+02],\n",
       "        [6.0584e+02, 2.8686e+02, 6.3884e+02, 3.5499e+02],\n",
       "        [1.2043e+02, 3.2339e+02, 1.4077e+02, 3.2976e+02],\n",
       "        [6.0580e+02, 2.8740e+02, 6.3900e+02, 3.5467e+02],\n",
       "        [6.0580e+02, 2.8740e+02, 6.3900e+02, 3.5467e+02],\n",
       "        [6.0598e+02, 2.8742e+02, 6.3904e+02, 3.5480e+02],\n",
       "        [6.0598e+02, 2.8742e+02, 6.3904e+02, 3.5480e+02],\n",
       "        [9.0472e+01, 3.2682e+02, 1.0879e+02, 3.3346e+02],\n",
       "        [9.0519e+01, 3.2631e+02, 1.1936e+02, 3.3366e+02],\n",
       "        [1.1433e+02, 3.2313e+02, 1.4086e+02, 3.3347e+02],\n",
       "        [2.1828e+02, 1.8179e+00, 3.2731e+02, 6.2628e+01],\n",
       "        [4.7609e+02, 1.3701e+02, 5.2557e+02, 2.1895e+02],\n",
       "        [4.4577e+02, 1.6757e+02, 4.9145e+02, 2.2273e+02],\n",
       "        [4.1256e+02, 1.6623e+02, 4.6193e+02, 2.3364e+02],\n",
       "        [4.4653e+02, 1.6736e+02, 5.1132e+02, 2.3414e+02],\n",
       "        [4.8080e+02, 1.4405e+02, 5.1767e+02, 2.7567e+02],\n",
       "        [4.1223e+02, 1.5720e+02, 4.6361e+02, 2.9501e+02],\n",
       "        [4.8897e+02, 1.7221e+02, 5.1201e+02, 2.8479e+02],\n",
       "        [4.2681e+02, 1.6433e+02, 4.6330e+02, 2.9396e+02],\n",
       "        [4.2981e+02, 1.6807e+02, 4.6637e+02, 2.9515e+02],\n",
       "        [4.4531e+02, 1.6816e+02, 5.1029e+02, 2.9408e+02],\n",
       "        [4.8817e+02, 1.7463e+02, 5.3726e+02, 2.8436e+02],\n",
       "        [4.0703e+02, 1.5935e+02, 4.6393e+02, 3.0030e+02],\n",
       "        [4.6223e+02, 2.2277e+02, 5.3878e+02, 2.8888e+02],\n",
       "        [5.5738e+02, 2.0658e+02, 6.3892e+02, 2.8874e+02],\n",
       "        [2.9043e+02, 2.1837e+02, 3.4973e+02, 3.1708e+02],\n",
       "        [2.1549e+02, 2.3176e+02, 2.9874e+02, 3.0380e+02],\n",
       "        [2.9039e+02, 2.1826e+02, 3.4954e+02, 3.1746e+02],\n",
       "        [1.3176e+02, 2.6194e+02, 2.1518e+02, 3.2986e+02],\n",
       "        [2.3371e-01, 2.7016e+02, 1.3579e+02, 3.3604e+02],\n",
       "        [8.4134e+01, 2.6247e+02, 2.1498e+02, 3.3440e+02],\n",
       "        [5.5060e+02, 3.0042e+02, 5.8551e+02, 4.0039e+02],\n",
       "        [6.1497e+00, 3.9039e+01, 1.7167e+02, 1.2807e+02],\n",
       "        [6.1497e+00, 3.9039e+01, 1.7167e+02, 1.2807e+02],\n",
       "        [6.1497e+00, 3.9039e+01, 1.7167e+02, 1.2807e+02],\n",
       "        [4.5362e+02, 1.4022e+02, 5.4060e+02, 2.8757e+02],\n",
       "        [6.1127e+00, 1.6552e+02, 1.5402e+02, 2.6764e+02],\n",
       "        [6.1127e+00, 1.6552e+02, 1.5402e+02, 2.6764e+02],\n",
       "        [6.1127e+00, 1.6552e+02, 1.5402e+02, 2.6764e+02],\n",
       "        [4.0800e+02, 1.5767e+02, 4.6364e+02, 2.9465e+02],\n",
       "        [4.2246e+02, 1.6075e+02, 4.8876e+02, 2.9483e+02],\n",
       "        [4.2246e+02, 1.6075e+02, 4.8876e+02, 2.9483e+02],\n",
       "        [4.2246e+02, 1.6075e+02, 4.8876e+02, 2.9483e+02],\n",
       "        [2.0306e+02, 2.1973e+02, 3.1994e+02, 3.0687e+02],\n",
       "        [9.0964e-02, 2.1953e+02, 2.1638e+02, 3.3314e+02],\n",
       "        [2.9206e+02, 2.1799e+02, 4.4558e+02, 3.1900e+02],\n",
       "        [4.1449e-01, 2.6208e+02, 2.1525e+02, 3.3829e+02],\n",
       "        [4.6154e+02, 3.5378e+02, 6.3947e+02, 4.2499e+02]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac565539-87c0-47b5-b7f1-db91a62a4af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57be185c-724c-4029-b5e9-c7ed883f1a92",
   "metadata": {},
   "source": [
    "### Run inference without internal forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b919e3d7-75f7-4e42-a0c8-da87d421b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_inference_wrapper(images, captions, visuql_queries, model):\n",
    "\n",
    "images = images.to(device)\n",
    "device = images.tensors.device\n",
    "visual_features = model.backbone(images.tensors)\n",
    "assert images.tensors.shape[0]==1 # TODO: Only query batch size = 1 for test\n",
    "labels_in_caption, all_map = model.get_labels_and_maps_from_positive_map(\n",
    "    positive_map=positive_map_label_to_token, dtype=visual_features[0].dtype)\n",
    "batched_labels_in_caption = [labels_in_caption]\n",
    "batched_all_map = [all_map]\n",
    "batched_pos_category_map = None\n",
    "batched_pos_labels = None    \n",
    "# query_features, query_attetion_masks, batched_has_vision_query=self.query_selector(\n",
    "#     batched_labels_in_caption, batched_all_map, batched_pos_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29cf435f-0de5-440c-8801-a45929ea4685",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_label_list, batched_location_map, batched_pos_labels = \\\n",
    "    batched_labels_in_caption, batched_all_map, batched_pos_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b03ad931-2613-49e9-9ebf-b644aa3cf3ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  6,\n",
       "  7,\n",
       "  8,\n",
       "  9,\n",
       "  10,\n",
       "  11,\n",
       "  12,\n",
       "  13,\n",
       "  14,\n",
       "  15,\n",
       "  16,\n",
       "  17,\n",
       "  18,\n",
       "  19,\n",
       "  20,\n",
       "  21,\n",
       "  22,\n",
       "  23,\n",
       "  24,\n",
       "  25,\n",
       "  26,\n",
       "  27,\n",
       "  28,\n",
       "  29,\n",
       "  30,\n",
       "  31,\n",
       "  32,\n",
       "  33,\n",
       "  34,\n",
       "  35,\n",
       "  36,\n",
       "  37,\n",
       "  38,\n",
       "  39,\n",
       "  40]]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def extract_visual_prompt_feature(t_image_list, box_list, model):\n",
    "    \"\"\"\n",
    "    image_list: ImageList\n",
    "    box_list: List[BoxList]\n",
    "    \"\"\"\n",
    "    images = to_image_list(image_list)\n",
    "    visual_features = model.backbone(image_list.tensors)    \n",
    "    query_feats=model.pooler(visual_features, box_list) # num_boxes, num_channels, pooler_size, pooler_size\n",
    "    query_feats=query_feats[None, ...] # 1, num_boxes, num_channels, pooler_size, pooler_size\n",
    "    query_feats = query_feats.mean(dim=[-2,-1]).permute(1, 0, 2) # num_boxes, num_scales, num_channels\n",
    "    # labels=torch.cat([t.get_field('labels') for t in box_list])\n",
    "    # max_query_number = cfg.VISION_QUERY.MAX_QUERY_NUMBER\n",
    "    # return query_feats, labels\n",
    "    return query_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ad85b4d4-d1db-4fec-bbe8-3703ba29f08e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3afcc741-ed82-4fe8-a544-3f68e185a8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 256])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aa68dc86-7b21-4600-97e2-3ce19d2670b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d2f51b2f-86e0-4ea3-9ccf-eb13a433408b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3333, 0.3333, 0.3333, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "        0.0000, 0.0000, 0.0000, 0.0000], device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_map[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "671aec40-14ab-4747-810e-296c1b83cca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_labels_in_caption[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
