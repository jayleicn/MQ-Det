{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17dae17d-44d4-4140-94ea-38da9818cda7",
   "metadata": {},
   "source": [
    "# Extract Visual Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e17476c7-04c5-41ae-aaba-3f72f888e19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15217208-dc48-4701-9766-fb0f5d646b37",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/layers/deform_conv.py:313: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/layers/deform_conv.py:378: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/layers/deform_conv.py:428: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/layers/set_loss.py:144: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/layers/set_loss.py:326: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/home/jielei/miniconda/envs/ov_det/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jielei/miniconda/envs/ov_det/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/rpn/loss.py:96: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/rpn/loss.py:231: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/rpn/loss.py:452: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/rpn/loss.py:849: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/roi_heads/box_head/inference.py:37: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/roi_heads/box_head/loss.py:122: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n",
      "/opt/hpcaas/.mounts/fs-0df31b178aa4037ac/home/jielei/MQ-Det/maskrcnn_benchmark/modeling/roi_heads/box_head/box_head.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd(cast_inputs=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# setup config\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from maskrcnn_benchmark.data import make_data_loader\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from maskrcnn_benchmark.data import transforms as T\n",
    "from maskrcnn_benchmark.config import try_to_find\n",
    "from maskrcnn_benchmark.structures.bounding_box import BoxList\n",
    "from maskrcnn_benchmark.utils.miscellaneous import mkdir, save_config\n",
    "from maskrcnn_benchmark.modeling.detector import build_detection_model\n",
    "from maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\n",
    "from maskrcnn_benchmark.structures.image_list import to_image_list\n",
    "from maskrcnn_benchmark.data.transforms import build_transforms # input image and box transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78f9b79-b086-4207-9be9-bda4fd5ca8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b21cb847-aebb-4e74-ae12-15e3e3b38cb1",
   "metadata": {},
   "source": [
    "### setup config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83fcd03c-86c9-42ea-b69a-33bb3659826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maskrcnn_benchmark.config import cfg as cfg_extract_vq\n",
    "# set other args\n",
    "cfg_extract_vq.local_rank = 0\n",
    "cfg_extract_vq.num_gpus = 1\n",
    "config_file = \"configs/pretrain/mq-glip-l.yaml\"\n",
    "cfg_extract_vq.merge_from_file(config_file)\n",
    "additional_model_config = \"configs/vision_query_5shot/lvis_minival.yaml\"\n",
    "cfg_extract_vq.merge_from_file(additional_model_config)\n",
    "\n",
    "opts = [\n",
    "    \"VISION_QUERY.QUERY_BANK_SAVE_PATH\", \"MODEL/lvis_query_5_pool7_sel_large.pth\",\n",
    "    \"VISION_QUERY.MAX_QUERY_NUMBER\", 5,\n",
    "    \"DATASETS.FEW_SHOT\", 5, \n",
    "    \"VISION_QUERY.QUERY_BANK_PATH\", \"\",\n",
    "    \"DATALOADER.NUM_WORKERS\", 0\n",
    "]\n",
    "cfg_extract_vq.merge_from_list(opts)\n",
    "\n",
    "cfg_extract_vq.freeze()\n",
    "\n",
    "# save_config(cfg_extract_vq, \"visualization/lvis_extract_visual_query_config_from_notebook.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8ce1d4-2eb9-4a4a-8d0c-36d4b5a2d9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = cfg_extract_vq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac9887-dbff-4fc6-8482-4434ea8c9668",
   "metadata": {},
   "source": [
    "### build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da022c61-d6d1-420f-be81-947f0f012739",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jielei/miniconda/envs/ov_det/lib/python3.10/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VISION BACKBONE USE GRADIENT CHECKPOINTING:  False\n",
      "LANGUAGE BACKBONE USE GRADIENT CHECKPOINTING:  False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of QVBertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['encoder.qv_layer.0.attn.norm.bias', 'encoder.qv_layer.0.attn.norm.weight', 'encoder.qv_layer.0.attn.norm_kv.bias', 'encoder.qv_layer.0.attn.norm_kv.weight', 'encoder.qv_layer.0.attn.to_kv.weight', 'encoder.qv_layer.0.attn.to_out.weight', 'encoder.qv_layer.0.attn.to_q.weight', 'encoder.qv_layer.0.attn_gate.linear1.weight', 'encoder.qv_layer.0.attn_gate.linear2.weight', 'encoder.qv_layer.0.attn_gate.norm.bias', 'encoder.qv_layer.0.attn_gate.norm.weight', 'encoder.qv_layer.0.ff.linear1.weight', 'encoder.qv_layer.0.ff.linear2.weight', 'encoder.qv_layer.0.ff.norm.bias', 'encoder.qv_layer.0.ff.norm.weight', 'encoder.qv_layer.0.ff_gate', 'encoder.qv_layer.1.attn.norm.bias', 'encoder.qv_layer.1.attn.norm.weight', 'encoder.qv_layer.1.attn.norm_kv.bias', 'encoder.qv_layer.1.attn.norm_kv.weight', 'encoder.qv_layer.1.attn.to_kv.weight', 'encoder.qv_layer.1.attn.to_out.weight', 'encoder.qv_layer.1.attn.to_q.weight', 'encoder.qv_layer.1.attn_gate.linear1.weight', 'encoder.qv_layer.1.attn_gate.linear2.weight', 'encoder.qv_layer.1.attn_gate.norm.bias', 'encoder.qv_layer.1.attn_gate.norm.weight', 'encoder.qv_layer.1.ff.linear1.weight', 'encoder.qv_layer.1.ff.linear2.weight', 'encoder.qv_layer.1.ff.norm.bias', 'encoder.qv_layer.1.ff.norm.weight', 'encoder.qv_layer.1.ff_gate', 'encoder.qv_layer.2.attn.norm.bias', 'encoder.qv_layer.2.attn.norm.weight', 'encoder.qv_layer.2.attn.norm_kv.bias', 'encoder.qv_layer.2.attn.norm_kv.weight', 'encoder.qv_layer.2.attn.to_kv.weight', 'encoder.qv_layer.2.attn.to_out.weight', 'encoder.qv_layer.2.attn.to_q.weight', 'encoder.qv_layer.2.attn_gate.linear1.weight', 'encoder.qv_layer.2.attn_gate.linear2.weight', 'encoder.qv_layer.2.attn_gate.norm.bias', 'encoder.qv_layer.2.attn_gate.norm.weight', 'encoder.qv_layer.2.ff.linear1.weight', 'encoder.qv_layer.2.ff.linear2.weight', 'encoder.qv_layer.2.ff.norm.bias', 'encoder.qv_layer.2.ff.norm.weight', 'encoder.qv_layer.2.ff_gate', 'encoder.qv_layer.3.attn.norm.bias', 'encoder.qv_layer.3.attn.norm.weight', 'encoder.qv_layer.3.attn.norm_kv.bias', 'encoder.qv_layer.3.attn.norm_kv.weight', 'encoder.qv_layer.3.attn.to_kv.weight', 'encoder.qv_layer.3.attn.to_out.weight', 'encoder.qv_layer.3.attn.to_q.weight', 'encoder.qv_layer.3.attn_gate.linear1.weight', 'encoder.qv_layer.3.attn_gate.linear2.weight', 'encoder.qv_layer.3.attn_gate.norm.bias', 'encoder.qv_layer.3.attn_gate.norm.weight', 'encoder.qv_layer.3.ff.linear1.weight', 'encoder.qv_layer.3.ff.linear2.weight', 'encoder.qv_layer.3.ff.norm.bias', 'encoder.qv_layer.3.ff.norm.weight', 'encoder.qv_layer.3.ff_gate', 'encoder.qv_layer.4.attn.norm.bias', 'encoder.qv_layer.4.attn.norm.weight', 'encoder.qv_layer.4.attn.norm_kv.bias', 'encoder.qv_layer.4.attn.norm_kv.weight', 'encoder.qv_layer.4.attn.to_kv.weight', 'encoder.qv_layer.4.attn.to_out.weight', 'encoder.qv_layer.4.attn.to_q.weight', 'encoder.qv_layer.4.attn_gate.linear1.weight', 'encoder.qv_layer.4.attn_gate.linear2.weight', 'encoder.qv_layer.4.attn_gate.norm.bias', 'encoder.qv_layer.4.attn_gate.norm.weight', 'encoder.qv_layer.4.ff.linear1.weight', 'encoder.qv_layer.4.ff.linear2.weight', 'encoder.qv_layer.4.ff.norm.bias', 'encoder.qv_layer.4.ff.norm.weight', 'encoder.qv_layer.4.ff_gate', 'encoder.qv_layer.5.attn.norm.bias', 'encoder.qv_layer.5.attn.norm.weight', 'encoder.qv_layer.5.attn.norm_kv.bias', 'encoder.qv_layer.5.attn.norm_kv.weight', 'encoder.qv_layer.5.attn.to_kv.weight', 'encoder.qv_layer.5.attn.to_out.weight', 'encoder.qv_layer.5.attn.to_q.weight', 'encoder.qv_layer.5.attn_gate.linear1.weight', 'encoder.qv_layer.5.attn_gate.linear2.weight', 'encoder.qv_layer.5.attn_gate.norm.bias', 'encoder.qv_layer.5.attn_gate.norm.weight', 'encoder.qv_layer.5.ff.linear1.weight', 'encoder.qv_layer.5.ff.linear2.weight', 'encoder.qv_layer.5.ff.norm.bias', 'encoder.qv_layer.5.ff.norm.weight', 'encoder.qv_layer.5.ff_gate', 'pre_select.layers.0.ff.linear1.weight', 'pre_select.layers.0.ff.linear2.weight', 'pre_select.layers.0.ff.norm.bias', 'pre_select.layers.0.ff.norm.weight', 'pre_select.layers.0.image_condition.norm.bias', 'pre_select.layers.0.image_condition.norm.weight', 'pre_select.layers.0.image_condition.norm_kv.bias', 'pre_select.layers.0.image_condition.norm_kv.weight', 'pre_select.layers.0.image_condition.to_kv.weight', 'pre_select.layers.0.image_condition.to_out.weight', 'pre_select.layers.0.image_condition.to_q.weight', 'pre_select.layers.1.ff.linear1.weight', 'pre_select.layers.1.ff.linear2.weight', 'pre_select.layers.1.ff.norm.bias', 'pre_select.layers.1.ff.norm.weight', 'pre_select.layers.1.image_condition.norm.bias', 'pre_select.layers.1.image_condition.norm.weight', 'pre_select.layers.1.image_condition.norm_kv.bias', 'pre_select.layers.1.image_condition.norm_kv.weight', 'pre_select.layers.1.image_condition.to_kv.weight', 'pre_select.layers.1.image_condition.to_out.weight', 'pre_select.layers.1.image_condition.to_q.weight', 'pre_select.layers.1.res_mapping.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n",
      "EARLY FUSION ON, USING MHA-B\n"
     ]
    }
   ],
   "source": [
    "# modified from tools.train_net.extract_query\n",
    "if cfg.DATASETS.FEW_SHOT:\n",
    "    assert cfg.DATASETS.FEW_SHOT == cfg.VISION_QUERY.MAX_QUERY_NUMBER, 'To extract the right query instances, set VISION_QUERY.MAX_QUERY_NUMBER = DATASETS.FEW_SHOT.'\n",
    "\n",
    "model = build_detection_model(cfg)\n",
    "device = torch.device(cfg.MODEL.DEVICE)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fa36b-96c1-475b-b4f9-36456c5631ca",
   "metadata": {},
   "source": [
    "### build dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532e2394-0010-4a45-a67f-7ad8e18743ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When using more than one image per GPU you may encounter an out-of-memory (OOM) error if your GPU does not have sufficient memory. If this happens, you can reduce SOLVER.IMS_PER_BATCH (for training) or TEST.IMS_PER_BATCH (for inference). For training, you must also adjust the learning rate and schedule length according to the linear scaling rule. See for example: https://github.com/facebookresearch/Detectron/blob/master/configs/getting_started/tutorial_1gpu_e2e_faster_rcnn_R-50-FPN.yaml#L14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The combined datasets are: ('lvis_grounding_train_for_obj365',).\n",
      "loading annotations into memory...\n",
      "Done (t=23.57s)\n",
      "creating index...\n",
      "index created!\n",
      "lvis_grounding_train_for_obj365 has the 3997 data points CocoGrounding_New\n"
     ]
    }
   ],
   "source": [
    "data_loader = make_data_loader(\n",
    "    cfg,\n",
    "    is_train=False,\n",
    "    is_cache=True,\n",
    "    is_distributed= cfg.num_gpus > 1,\n",
    ")\n",
    "assert isinstance(data_loader, list) and len(data_loader)==1\n",
    "data_loader = data_loader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d2f07f7e-526c-4e77-9181-d890138ac11b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CocoGrounding_New\n",
       "    Number of datapoints: 3997\n",
       "    Root location: ./DATASET/coco"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07ea90b-8213-401a-a00b-81e06dc7b568",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cfg.SOLVER.IMS_PER_BATCH: 8\n",
      "data_loader.batch_size: None\n",
      "len(data_loader): 500\n",
      "------------------------------\n",
      "cfg.DATASETS:\n",
      "ADD_DET_PROMPT: False\n",
      "ADD_DET_PROMPT_ADVANCED: False\n",
      "ADD_NORMED_CXCY: False\n",
      "ALTERNATIVE_TRAINING: False\n",
      "BING_INDEX_LIST: []\n",
      "BOX_THRESHOLD: 0.1\n",
      "CAPTION_CONF: 0.5\n",
      "CAPTION_FORMAT_VERSION: v2\n",
      "CAPTION_MIN_BOX: 1\n",
      "CAPTION_NMS: -1.0\n",
      "CAPTION_PROMPT: None\n",
      "CLASS_AGNOSTIC: False\n",
      "CLASS_CONCAT: False\n",
      "COCO_COPY: 1\n",
      "CONTROL_PROB: (0.0, 0.0, 0.5, 0.0)\n",
      "DETECT_NONEXIST: False\n",
      "DISABLE_CLIP_TO_IMAGE: False\n",
      "DISABLE_SHUFFLE: False\n",
      "DIVER_BOX_FOR_VQA: False\n",
      "EXCLUDE_CROWD: True\n",
      "FEW_SHOT: 5\n",
      "FLICKR_COPY: 8\n",
      "FLICKR_GT_TYPE: separate\n",
      "FULL_QUESTION_PROB: 0.5\n",
      "FURTHER_SCREEN: True\n",
      "GENERAL_COPY: -1\n",
      "GENERAL_COPY_TEST: -1\n",
      "INFERENCE_CAPTION: False\n",
      "IN_COPY: 2\n",
      "LOCAL_DEBUG: False\n",
      "LVIS_COPY: 1\n",
      "LVIS_USE_NORMAL_AP: False\n",
      "MAX_BOX: -1\n",
      "MIXED_COPY: 4\n",
      "MULTISTAGE_TRAINING: False\n",
      "NEG_QUESTION_PROB: 0.8\n",
      "NO_MINUS_ONE_FOR_ONE_HOT: False\n",
      "NO_RANDOM_PACK_PROBABILITY: 0.4\n",
      "OBJECT365_COPY: 2\n",
      "OI_COPY: 1\n",
      "ONE_HOT: False\n",
      "OVERRIDE_CATEGORY: None\n",
      "PACK_RANDOM_CAPTION_NUMBER: 20\n",
      "POS_QUESTION_PROB: 0.6\n",
      "PREDEFINED_TEXT: None\n",
      "PROMPT_LIMIT_NEG: -1\n",
      "PROMPT_VERSION: \n",
      "RANDOM_PACK_PROB: 0.5\n",
      "RANDOM_SAMPLE_NEG: 85\n",
      "REGISTER:\n",
      "  lvis_evaluation_mini_val:\n",
      "    ann_file: coco/annotations/lvis_v1_minival_inserted_image_name.json\n",
      "    img_dir: coco\n",
      "  lvis_evaluation_val:\n",
      "    ann_file: coco/annotations/lvis_od_val.json\n",
      "    img_dir: coco\n",
      "REPLACE_CLEAN_LABEL: False\n",
      "SAFEGUARD_POSITIVE_CAPTION: True\n",
      "SAMPLE_NEGATIVE_FOR_GROUNDING_DATA: -1.0\n",
      "SAMPLE_RATIO: 0.0\n",
      "SEPARATION_TOKENS: . \n",
      "SEP_AT_LAST: False\n",
      "SHUFFLE_SEED: 0\n",
      "SPECIAL_SAFEGUARD_FOR_COCO_GROUNDING: True\n",
      "SUPRESS_QUERY: None\n",
      "TEST: ('lvis_evaluation_mini_val',)\n",
      "TEST_DATASETNAME_SUFFIX: \n",
      "TRAIN: ('lvis_grounding_train_for_obj365',)\n",
      "TRAIN_DATASETNAME_SUFFIX: \n",
      "USE_CAPTION_PROMPT: False\n",
      "USE_COCO_FORMAT: False\n",
      "USE_CROWD: False\n",
      "USE_OD_AUG: False\n",
      "USE_OVERRIDE_CATEGORY: True\n",
      "USE_SUPRESS_QUERY: False\n",
      "VG_COPY: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"cfg.SOLVER.IMS_PER_BATCH: {cfg.SOLVER.IMS_PER_BATCH}\")\n",
    "print(f\"data_loader.batch_size: {data_loader.batch_size}\")\n",
    "print(f\"len(data_loader): {len(data_loader)}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"cfg.DATASETS:\\n{cfg.DATASETS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "13e2048f-fac3-4b95-ae9f-b3816b454718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'NUM_WORKERS': 0, 'SIZE_DIVISIBILITY': 32, 'ASPECT_RATIO_GROUPING': False, 'MIN_KPS_PER_IMS': 0, 'USE_RANDOM_SEED': False, 'DISTRIBUTE_CHUNK_AMONG_NODE': False})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.DATALOADER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01939ce-9188-46fe-a81f-76b8182bba16",
   "metadata": {},
   "source": [
    "#### check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0e704f4-ec2d-4f55-bb22-adc5b77e53c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch data: (<maskrcnn_benchmark.structures.image_list.ImageList object at 0x7d468fc7a2f0>, (BoxList(num_boxes=2, image_width=1196, image_height=800, mode=xyxy), BoxList(num_boxes=1, image_width=1204, image_height=800, mode=xyxy), BoxList(num_boxes=6, image_width=800, image_height=1064, mode=xyxy), BoxList(num_boxes=6, image_width=1071, image_height=800, mode=xyxy), BoxList(num_boxes=14, image_width=800, image_height=1049, mode=xyxy), BoxList(num_boxes=3, image_width=1049, image_height=800, mode=xyxy), BoxList(num_boxes=3, image_width=800, image_height=1066, mode=xyxy), BoxList(num_boxes=1, image_width=1201, image_height=800, mode=xyxy)), (0, 1, 2, 3, 4, 5, 6, 7), tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), None, None)\n"
     ]
    }
   ],
   "source": [
    "# Get a single batch\n",
    "batch_iterator = iter(data_loader)\n",
    "batch_data = next(batch_iterator)\n",
    "print(\"Batch data:\", batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf6cbf8-3fd4-4d72-ae9f-5d20b9d487fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets, *_ = batch_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1043fa-a054-4272-929b-3c84ac4e43a8",
   "metadata": {},
   "source": [
    " 1. check images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34263c7b-bdc1-498f-989a-10cdc6c67762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repr(images):\n",
      "<maskrcnn_benchmark.structures.image_list.ImageList object at 0x7d468fc7a2f0>\n",
      "------------------------------\n",
      "images.tensors.shape:\n",
      "torch.Size([8, 3, 1088, 1216])\n",
      "------------------------------\n",
      "images.image_sizes:\n",
      "[torch.Size([800, 1196]), torch.Size([800, 1204]), torch.Size([1064, 800]), torch.Size([800, 1071]), torch.Size([1049, 800]), torch.Size([800, 1049]), torch.Size([1066, 800]), torch.Size([800, 1201])]\n"
     ]
    }
   ],
   "source": [
    "print(f\"repr(images):\\n{repr(images)}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"images.tensors.shape:\\n{images.tensors.shape}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"images.image_sizes:\\n{images.image_sizes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f108f26-f1be-499f-8513-936d61e2a518",
   "metadata": {},
   "source": [
    "2. check targets (boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06452bea-6d30-43c9-8bce-5bd0cb047651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repr(targets):\n",
      "(BoxList(num_boxes=2, image_width=1196, image_height=800, mode=xyxy), BoxList(num_boxes=1, image_width=1204, image_height=800, mode=xyxy), BoxList(num_boxes=6, image_width=800, image_height=1064, mode=xyxy), BoxList(num_boxes=6, image_width=1071, image_height=800, mode=xyxy), BoxList(num_boxes=14, image_width=800, image_height=1049, mode=xyxy), BoxList(num_boxes=3, image_width=1049, image_height=800, mode=xyxy), BoxList(num_boxes=3, image_width=800, image_height=1066, mode=xyxy), BoxList(num_boxes=1, image_width=1201, image_height=800, mode=xyxy))\n",
      "------------------------------\n",
      "repr(targets[0]):\n",
      "BoxList(num_boxes=2, image_width=1196, image_height=800, mode=xyxy)\n",
      "------------------------------\n",
      "targets[0].bbox:\n",
      "tensor([[380.9447,  64.7477, 855.8875, 413.8505],\n",
      "        [451.7142, 289.8131, 760.7495, 653.2710]])\n",
      "------------------------------\n",
      "targets[0].size:\n",
      "(1196, 800)\n",
      "------------------------------\n",
      "targets[0].fields():\n",
      "['labels', 'boxes', 'caption', 'image_id', 'tokens_positive', 'area', 'iscrowd', 'orig_size', 'size', 'positive_map', 'all_map', 'labels_in_caption', 'positive_category_map', 'original_od_label']\n",
      "------------------------------\n",
      "targets[0].get_field('labels'):\n",
      "tensor([ 461, 1139])\n"
     ]
    }
   ],
   "source": [
    "print(f\"repr(targets):\\n{repr(targets)}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"repr(targets[0]):\\n{repr(targets[0])}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"targets[0].bbox:\\n{targets[0].bbox}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"targets[0].size:\\n{targets[0].size}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"targets[0].fields():\\n{targets[0].fields()}\")\n",
    "print(\"-\"*30)\n",
    "print(f\"targets[0].get_field('labels'):\\n{targets[0].get_field('labels')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddb45aa-7b06-4ae0-aae9-8b59f0d2d66a",
   "metadata": {},
   "source": [
    "### Run inference -- use internal function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfde686c-e65b-4597-8d8d-4b6311fb12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_images=defaultdict(list)\n",
    "query_images = model.extract_query(images.to(device), targets, query_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8090697-b708-484d-b83a-8586fa369e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_images.keys() dict_keys([461, 1139, 1202, 715, 959, 1045, 547, 1023, 911, 298, 837, 43, 900, 1142, 1017, 544, 422, 800, 876, 1056, 192])\n",
      "query_images[461].shape torch.Size([1, 1, 256])\n",
      "query_images[461][0, 0, :10]: tensor([ 0.5390, -0.6056,  0.0860, -0.1750, -1.6917,  0.1566, -0.6254,  0.2460,\n",
      "         0.3537,  1.0738], device='cuda:0')\n",
      "len(query_images): 21\n",
      "------------------------------\n",
      "class 461, v.shape torch.Size([1, 1, 256])\n",
      "class 1139, v.shape torch.Size([1, 1, 256])\n",
      "class 1202, v.shape torch.Size([1, 1, 256])\n",
      "class 715, v.shape torch.Size([1, 1, 256])\n",
      "class 959, v.shape torch.Size([1, 1, 256])\n",
      "class 1045, v.shape torch.Size([1, 1, 256])\n",
      "class 547, v.shape torch.Size([1, 1, 256])\n",
      "class 1023, v.shape torch.Size([2, 1, 256])\n",
      "class 911, v.shape torch.Size([5, 1, 256])\n",
      "class 298, v.shape torch.Size([3, 1, 256])\n",
      "class 837, v.shape torch.Size([1, 1, 256])\n",
      "class 43, v.shape torch.Size([3, 1, 256])\n",
      "class 900, v.shape torch.Size([2, 1, 256])\n",
      "class 1142, v.shape torch.Size([3, 1, 256])\n",
      "class 1017, v.shape torch.Size([2, 1, 256])\n",
      "class 544, v.shape torch.Size([1, 1, 256])\n",
      "class 422, v.shape torch.Size([2, 1, 256])\n",
      "class 800, v.shape torch.Size([1, 1, 256])\n",
      "class 876, v.shape torch.Size([1, 1, 256])\n",
      "class 1056, v.shape torch.Size([1, 1, 256])\n",
      "class 192, v.shape torch.Size([1, 1, 256])\n"
     ]
    }
   ],
   "source": [
    "print(f\"query_images.keys() {query_images.keys()}\")\n",
    "print(f\"query_images[461].shape {query_images[461].shape}\")\n",
    "print(f\"query_images[461][0, 0, :10]: {query_images[461][0, 0, :10]}\")\n",
    "print(f\"len(query_images): {len(query_images)}\")\n",
    "print(\"-\"*30)\n",
    "for k, v in query_images.items():\n",
    "    print(f\"class {k}, v.shape {v.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aee3a7-fec5-4fdf-b6ad-c534ff7b2b8b",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "modified from `model.extract_query` from the cell above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aae8034e-0b96-4e3d-884c-aafefeb8a87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "raw_targets = copy.deepcopy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f56fbc9f-52b5-4d1f-806f-681458e98914",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.to(device)\n",
    "targets = [target.to(device)\n",
    "            for target in targets if target is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b93ee869-cd91-4ceb-8190-e4014a0f8f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<maskrcnn_benchmark.structures.image_list.ImageList at 0x7d48262631f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c19e5d34-0439-4fc1-b5a9-aa6469791880",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    images = to_image_list(images)\n",
    "    assert 'vl' not in cfg.MODEL.SWINT.VERSION, 'Only support vision inputs now'\n",
    "    visual_features = model.backbone(images.tensors)    \n",
    "    query_feats=model.pooler(visual_features, targets) # num_boxes, num_channels, pooler_size, pooler_size\n",
    "    query_feats=query_feats[None, ...] # 1, num_boxes, num_channels, pooler_size, pooler_size\n",
    "    query_feats = query_feats.mean(dim=[-2,-1]).permute(1, 0, 2) # num_boxes, num_scales, num_channels\n",
    "    labels=torch.cat([t.get_field('labels') for t in targets])\n",
    "    assert len(labels)==len(query_feats)\n",
    "    max_query_number = cfg.VISION_QUERY.MAX_QUERY_NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "2d57dbf4-d91b-4619-b32d-eb7ffe0f3d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_feats.shape: torch.Size([36, 1, 256])\n",
      "len(labels): 36\n",
      "labels tensor([ 461, 1139, 1202,  715,  959, 1045,  547, 1023, 1023,  911,  911,  911,\n",
      "         911,  911,  911,  298,  298,  298,  837,   43,   43,   43,  900,  900,\n",
      "        1142, 1142, 1142, 1017, 1017,  544,  422,  422,  800,  876, 1056,  192],\n",
      "       device='cuda:0')\n",
      "type(visual_features): <class 'tuple'>\n",
      "visual_features[0].shape: torch.Size([8, 256, 136, 152])\n"
     ]
    }
   ],
   "source": [
    "print(f\"query_feats.shape: {query_feats.shape}\")\n",
    "print(f\"len(labels): {len(labels)}\")\n",
    "print(f\"labels {labels}\")\n",
    "print(f\"type(visual_features): {type(visual_features)}\")\n",
    "print(f\"visual_features[0].shape: {visual_features[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce2754d-e0af-4a03-98d7-cf613cb12475",
   "metadata": {},
   "source": [
    "### Run inference with loaded single image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd43977-6500-4ab4-9f65-1eb7a48d16c8",
   "metadata": {},
   "source": [
    "##### Load and transform image and boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "95e7ce22-b46b-470a-a7c7-87089b882d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_eval_transform(cfg):\n",
    "    min_size = cfg.INPUT.MIN_SIZE_TEST\n",
    "    max_size = cfg.INPUT.MAX_SIZE_TEST\n",
    "    flip_horizontal_prob = 0.0\n",
    "    fix_res = cfg.INPUT.FIX_RES # false\n",
    "    if cfg.INPUT.FORMAT != '':\n",
    "        input_format = cfg.INPUT.FORMAT\n",
    "    elif cfg.INPUT.TO_BGR255:\n",
    "        input_format = 'bgr255'\n",
    "    normalize_transform = T.Normalize(\n",
    "        mean=cfg.INPUT.PIXEL_MEAN, std=cfg.INPUT.PIXEL_STD, format=input_format\n",
    "    )\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.Resize(min_size, max_size, restrict=fix_res),\n",
    "            T.RandomHorizontalFlip(flip_horizontal_prob),\n",
    "            T.ToTensor(),\n",
    "            normalize_transform,\n",
    "        ]\n",
    "    )\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56edbdf7-730a-4c71-8740-0e36b69e7045",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_transform = build_eval_transform(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "8309d4ea-7f8c-44f3-ab73-8cbca4d9b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### load image from path, load boxes\n",
    "image_path = os.path.join(\n",
    "    data_loader.dataset.root, \n",
    "    data_loader.dataset.coco.loadImgs(data_id)[0][\"file_name\"]\n",
    ")\n",
    "loaded_image = Image.open(image_path).convert(\"RGB\")\n",
    "# loaded_boxes = dxata_loader.dataset.coco.loadAnns(data_loader.dataset.coco.getAnnIds(data_id))\n",
    "# tgt = [obj for obj in loaded_boxes if obj[\"iscrowd\"] == 0]\n",
    "# boxes = [obj[\"bbox\"] for obj in tgt]\n",
    "# data from the above commented code\n",
    "loaded_boxes = [\n",
    "    [203.85, 34.64, 255.15, 187.77],\n",
    "    [241.72, 155.05, 166.37, 195.45]\n",
    "]\n",
    "loaded_boxes = torch.as_tensor(loaded_boxes).reshape(-1, 4)  # guard against no boxes\n",
    "loaded_target = BoxList(loaded_boxes, loaded_image.size, mode=\"xywh\").convert(\"xyxy\")\n",
    "\n",
    "anno = data_loader.dataset.coco.loadAnns(data_loader.dataset.coco.getAnnIds(data_id))\n",
    "labels = torch.tensor([e[\"category_id\"] for e in anno])\n",
    "loaded_target.add_field(\"labels\", labels)\n",
    "\n",
    "#### transform\n",
    "t_image, t_target = eval_transform(loaded_image, loaded_target)\n",
    "t_image = t_image.to(device)\n",
    "t_image_list = to_image_list(tensors=t_image[None, :], size_divisible=cfg.DATALOADER.SIZE_DIVISIBILITY)\n",
    "t_target = [t_target.to(device), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b0ed4704-5ef3-4793-aa20-b9e8bc51b05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[380.9447,  64.7477, 855.8875, 413.8505],\n",
       "        [451.7142, 289.8131, 760.7495, 653.2710]], device='cuda:0')"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_target[0].bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "682eb079-a278-40be-8d11-97cb8dbbf57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.3164, -1.2990, -1.2816,  ..., -0.9156, -0.9330, -0.9330],\n",
       "         [-1.3164, -1.2990, -1.2816,  ..., -0.9156, -0.9330, -0.9330],\n",
       "         [-1.2990, -1.2990, -1.2990,  ..., -0.9156, -0.9330, -0.9330],\n",
       "         ...,\n",
       "         [ 1.2980,  1.2980,  1.2980,  ...,  1.4897,  1.4897,  1.4897],\n",
       "         [ 1.2805,  1.2805,  1.2980,  ...,  1.4897,  1.4897,  1.4897],\n",
       "         [ 1.2805,  1.2805,  1.2980,  ...,  1.4897,  1.4897,  1.4897]],\n",
       "\n",
       "        [[-1.4755, -1.4580, -1.4405,  ..., -0.3725, -0.3550, -0.3550],\n",
       "         [-1.4755, -1.4580, -1.4405,  ..., -0.3725, -0.3550, -0.3550],\n",
       "         [-1.4580, -1.4580, -1.4580,  ..., -0.3725, -0.3550, -0.3550],\n",
       "         ...,\n",
       "         [ 1.0980,  1.0980,  1.0980,  ...,  1.2206,  1.2206,  1.2206],\n",
       "         [ 1.0805,  1.0805,  1.0980,  ...,  1.2206,  1.2206,  1.2206],\n",
       "         [ 1.0805,  1.0805,  1.0980,  ...,  1.2206,  1.2206,  1.2206]],\n",
       "\n",
       "        [[-1.6555, -1.6555, -1.6384,  ..., -0.9877, -0.9705, -0.9705],\n",
       "         [-1.6555, -1.6555, -1.6384,  ..., -0.9877, -0.9705, -0.9705],\n",
       "         [-1.6384, -1.6384, -1.6555,  ..., -0.9877, -0.9705, -0.9705],\n",
       "         ...,\n",
       "         [ 0.9132,  0.9132,  0.9132,  ...,  0.9817,  0.9817,  0.9817],\n",
       "         [ 0.8961,  0.8961,  0.9132,  ...,  0.9817,  0.9817,  0.9817],\n",
       "         [ 0.8961,  0.8961,  0.9132,  ...,  0.9817,  0.9817,  0.9817]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e88d02-d9cc-4efb-b1a3-0ce565bca380",
   "metadata": {},
   "source": [
    "##### Check against direct loading from data_loader.dataset\n",
    "Image and boxes match exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fe31ace7-1101-4ad2-923f-d02d76db14d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.3164, -1.2990, -1.2816,  ..., -0.9156, -0.9330, -0.9330],\n",
       "          [-1.3164, -1.2990, -1.2816,  ..., -0.9156, -0.9330, -0.9330],\n",
       "          [-1.2990, -1.2990, -1.2990,  ..., -0.9156, -0.9330, -0.9330],\n",
       "          ...,\n",
       "          [ 1.2980,  1.2980,  1.2980,  ...,  1.4897,  1.4897,  1.4897],\n",
       "          [ 1.2805,  1.2805,  1.2980,  ...,  1.4897,  1.4897,  1.4897],\n",
       "          [ 1.2805,  1.2805,  1.2980,  ...,  1.4897,  1.4897,  1.4897]],\n",
       " \n",
       "         [[-1.4755, -1.4580, -1.4405,  ..., -0.3725, -0.3550, -0.3550],\n",
       "          [-1.4755, -1.4580, -1.4405,  ..., -0.3725, -0.3550, -0.3550],\n",
       "          [-1.4580, -1.4580, -1.4580,  ..., -0.3725, -0.3550, -0.3550],\n",
       "          ...,\n",
       "          [ 1.0980,  1.0980,  1.0980,  ...,  1.2206,  1.2206,  1.2206],\n",
       "          [ 1.0805,  1.0805,  1.0980,  ...,  1.2206,  1.2206,  1.2206],\n",
       "          [ 1.0805,  1.0805,  1.0980,  ...,  1.2206,  1.2206,  1.2206]],\n",
       " \n",
       "         [[-1.6555, -1.6555, -1.6384,  ..., -0.9877, -0.9705, -0.9705],\n",
       "          [-1.6555, -1.6555, -1.6384,  ..., -0.9877, -0.9705, -0.9705],\n",
       "          [-1.6384, -1.6384, -1.6555,  ..., -0.9877, -0.9705, -0.9705],\n",
       "          ...,\n",
       "          [ 0.9132,  0.9132,  0.9132,  ...,  0.9817,  0.9817,  0.9817],\n",
       "          [ 0.8961,  0.8961,  0.9132,  ...,  0.9817,  0.9817,  0.9817],\n",
       "          [ 0.8961,  0.8961,  0.9132,  ...,  0.9817,  0.9817,  0.9817]]]),\n",
       " BoxList(num_boxes=2, image_width=1196, image_height=800, mode=xyxy),\n",
       " 0)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "103bae9a-adbf-411b-a9ec-3a9d233edd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[380.9447,  64.7477, 855.8875, 413.8505],\n",
       "        [451.7142, 289.8131, 760.7495, 653.2710]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.dataset[0][1].bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6e94894e-87f4-49d3-bec0-4d3ada99703c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(data_loader.dataset[0][0].to(device) == t_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5721f93d-88cb-44c6-845c-01af034513c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(data_loader.dataset[0][1].bbox == t_target[0].bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77802557-4370-4b11-ab06-1a09c3174cfb",
   "metadata": {},
   "source": [
    "#### Run inference on loaded image and boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb1317d5-3a06-4107-847a-de881b3c8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input is t_image and t_target\n",
    "@torch.no_grad()\n",
    "def run_inference_with_image_and_boxes(t_image_list, box_list, model, cfg):\n",
    "    \"\"\"\n",
    "    image_list: ImageList\n",
    "    box_list: List[BoxList]\n",
    "    \"\"\"\n",
    "    images = to_image_list(image_list)\n",
    "    assert 'vl' not in cfg.MODEL.SWINT.VERSION, 'Only support vision inputs now'\n",
    "    visual_features = model.backbone(image_list.tensors)    \n",
    "    query_feats=model.pooler(visual_features, box_list) # num_boxes, num_channels, pooler_size, pooler_size\n",
    "    query_feats=query_feats[None, ...] # 1, num_boxes, num_channels, pooler_size, pooler_size\n",
    "    query_feats = query_feats.mean(dim=[-2,-1]).permute(1, 0, 2) # num_boxes, num_scales, num_channels\n",
    "    labels=torch.cat([t.get_field('labels') for t in box_list])\n",
    "    assert len(labels)==len(query_feats)\n",
    "    max_query_number = cfg.VISION_QUERY.MAX_QUERY_NUMBER\n",
    "    return query_feats, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "cc702ee9-dd2a-4be0-b8a7-c897bd55baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query_feats, new_labels = run_inference_with_image_and_boxes(t_image_list, t_target, model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "3f6547da-effb-4c78-a542-88ccce4e66ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 461, 1139], device='cuda:0')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ce379969-dc67-4cdb-896c-a9f98617ae9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 256])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_query_feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2eb68-cb02-4f32-8ef9-94ef192cc1b1",
   "metadata": {},
   "source": [
    "##### check inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c0f94264-3450-4e1f-a956-fed48cbf8f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36, 1, 256])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original\n",
    "query_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d4ae770e-e005-48c7-8c70-bb49c1bf9d49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(query_feats[:2] == new_query_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5d829c8c-f6c7-4965-ae04-8d24273866b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8489,  0.4200,  0.4687, -0.8887, -1.1715,  1.8230,  0.6481,  0.3095,\n",
       "        -0.7059,  0.7495], device='cuda:0')"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_feats[0, 0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "16526e3d-fc8e-445b-985e-f49f4201bc26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8383,  0.4312,  0.4734, -0.8855, -1.1696,  1.8102,  0.6440,  0.3118,\n",
       "        -0.7052,  0.7440], device='cuda:0')"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_query_feats[0, 0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fa393-c6fe-4d14-9fdc-39ae82ad036c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9803a4a2-bab2-463e-a95c-fb6ddc468317",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query_feats2, new_labels2 = run_inference_with_image_and_boxes(t_image_list, t_target, model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a755734a-4a4a-4a93-a0f5-5a741686502e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8383,  0.4312,  0.4734, -0.8855, -1.1696,  1.8102,  0.6440,  0.3118,\n",
       "        -0.7052,  0.7440], device='cuda:0')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_query_feats2[0, 0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "774f2703-8629-488e-9168-2a3fb5d255d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32, 800, 1196])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_image_list.tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8865cbd7-2c86-48ac-ae84-9abfcb2ed328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='cuda:0')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note the batched version input images will have paddingm so there will be some differences in output\n",
    "torch.all(t_image_list.tensors == images.tensors[0:1, :, :t_image_list.tensors.shape[2], :t_image_list.tensors.shape[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d0f28454-6123-4cbf-ad80-8b34a7c7719b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.3164, -1.2990, -1.2816,  ..., -0.9156, -0.9330, -0.9330],\n",
       "          [-1.3164, -1.2990, -1.2816,  ..., -0.9156, -0.9330, -0.9330],\n",
       "          [-1.2990, -1.2990, -1.2990,  ..., -0.9156, -0.9330, -0.9330],\n",
       "          ...,\n",
       "          [ 1.2980,  1.2980,  1.2980,  ...,  1.4897,  1.4897,  1.4897],\n",
       "          [ 1.2805,  1.2805,  1.2980,  ...,  1.4897,  1.4897,  1.4897],\n",
       "          [ 1.2805,  1.2805,  1.2980,  ...,  1.4897,  1.4897,  1.4897]],\n",
       "\n",
       "         [[-1.4755, -1.4580, -1.4405,  ..., -0.3725, -0.3550, -0.3550],\n",
       "          [-1.4755, -1.4580, -1.4405,  ..., -0.3725, -0.3550, -0.3550],\n",
       "          [-1.4580, -1.4580, -1.4580,  ..., -0.3725, -0.3550, -0.3550],\n",
       "          ...,\n",
       "          [ 1.0980,  1.0980,  1.0980,  ...,  1.2206,  1.2206,  1.2206],\n",
       "          [ 1.0805,  1.0805,  1.0980,  ...,  1.2206,  1.2206,  1.2206],\n",
       "          [ 1.0805,  1.0805,  1.0980,  ...,  1.2206,  1.2206,  1.2206]],\n",
       "\n",
       "         [[-1.6555, -1.6555, -1.6384,  ..., -0.9877, -0.9705, -0.9705],\n",
       "          [-1.6555, -1.6555, -1.6384,  ..., -0.9877, -0.9705, -0.9705],\n",
       "          [-1.6384, -1.6384, -1.6555,  ..., -0.9877, -0.9705, -0.9705],\n",
       "          ...,\n",
       "          [ 0.9132,  0.9132,  0.9132,  ...,  0.9817,  0.9817,  0.9817],\n",
       "          [ 0.8961,  0.8961,  0.9132,  ...,  0.9817,  0.9817,  0.9817],\n",
       "          [ 0.8961,  0.8961,  0.9132,  ...,  0.9817,  0.9817,  0.9817]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_image_list.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "47244b49-d4f1-4eb7-b586-998e535c47db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.3164, -1.2990, -1.2816,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.3164, -1.2990, -1.2816,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.2990, -1.2990, -1.2990,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.4755, -1.4580, -1.4405,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.4755, -1.4580, -1.4405,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.4580, -1.4580, -1.4580,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[-1.6555, -1.6555, -1.6384,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.6555, -1.6555, -1.6384,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-1.6384, -1.6384, -1.6555,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note the zero paddings\n",
    "images.tensors[0:1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
